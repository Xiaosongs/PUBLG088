---
title: "PUBLG088 Lab 7: Nonlinear Models"
author: "Slava Mikhaylov"
output: html_document
---


This lab session is based on lab exercises in James et al. [Chapter 7].

We begin by loading that `ISLR` package and attaching to the `Wage` dataset that we will be using throughout this exercise.

```{r, message=FALSE, warning=FALSE}
library(ISLR)
attach(Wage)
```

### 7.1 Polynomial Regression and Step Functions

Let's fit a linear model to predict `wage` with a forth-degree polynomial using the [`poly()`](http://bit.ly/R_poly)

```{r}
fit <- lm(wage ~ poly(age, 4), data = Wage)
coef(summary(fit))
```

We can also obtain raw instead of orthogonal polynomials using the `raw = TRUE` argument to [`poly()`](http://bit.ly/R_poly)

```{r}
fit2 <- lm(wage ~ poly(age, 4, raw = TRUE), data = Wage)
coef(summary(fit2))
```

Finally, instead of using [`poly()`](http://bit.ly/R_poly), we can specify the polynomials directly in the formula as shown below.

```{r}
fit2a <- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data = Wage)
coef(fit2a)
```

A more compact version of the same example uses [`cbind()`](http://bit.ly/R_cbind) and eliminates the need to wrap each term in [`I()`](http://bit.ly/R_asis).

```{r}
fit2b <- lm(wage ~ cbind(age, age^2, age^3, age^4), data = Wage)
```

We can create an age grid for the targeted values of the prediction and pass the grid to [`predict()`](http://bit.ly/R_predict).

```{r}
agelims <- range(age)
age.grid <- seq(from = agelims[1], to = agelims[2])
preds <- predict(fit, newdata = list(age = age.grid), se = TRUE)
se.bands <- cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)
```

```{r}
preds2 <- predict(fit2, newdata = list(age = age.grid), se = TRUE)
max(abs(preds$fit - preds2$fit))
```

We can use [`anova()`](http://bit.ly/R_anova) to compare five different models of linear fit.

```{r}
fit.1 <- lm(wage ~ age, data = Wage)
fit.2 <- lm(wage ~ poly(age, 2), data = Wage)
fit.3 <- lm(wage ~ poly(age, 3), data = Wage)
fit.4 <- lm(wage ~ poly(age, 4), data = Wage)
fit.5 <- lm(wage ~ poly(age, 5), data = Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```

The same p-values can also be obtained from the [`coef()`](http://bit.ly/R_coef) function.

```{r}
coef(summary(fit.5))
```

The [`anova()`](http://bit.ly/R_anova) function can also compare the variances when other terms are included as predictors.

```{r}
fit.1 <- lm(wage ~ education + age, data = Wage)
fit.2 <- lm(wage ~ education + poly(age, 2), data = Wage)
fit.3 <- lm(wage ~ education + poly(age, 3), data = Wage)
anova(fit.1, fit.2, fit.3)
```

With [`glm()`](http://bit.ly/R_glm) we can also fit a polynomial logistic regression.

```{r}
fit <- glm(I(wage > 250) ~ poly(age, 4), data = Wage, family = binomial)
```

And use the same method for making predictions using [`predict()`](http://bit.ly/R_predict) as seen above.

```{r}
preds <- predict(fit, newdata = list(age = age.grid), se = TRUE)
```


```{r}
pfit <- exp(preds$fit)/(1 + exp(preds$fit))
se.bands.logit <- cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)
se.bands <- exp(se.bands.logit)/(1 + exp(se.bands.logit))
```


```{r}
preds <- predict(fit, newdata = list(age = age.grid), type = "response", se = TRUE)
```

We can plot these results with the [`plot()`](http://bit.ly/R_plot) function as usual.

```{r}
par(mfrow = c(1, 2), mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0))
plot(age, wage, xlim = agelims, cex = 0.5, col = "darkgrey")
title("Degree -4 Polynomial ", outer = TRUE)
lines(age.grid, preds$fit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)

plot(age, I(wage > 250), xlim = agelims, type = "n", ylim = c(0, 0.2))
points(jitter(age), I((wage > 250)/5), cex = 0.5, pch = "|", col = " darkgrey ")
lines(age.grid, pfit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
```

In the above plot, the [`jitter()`](http://bit.ly/R_jitter) function is used to prevent the same age observations from overlapping each other.

The [`cut()`](http://bit.ly/R_cut) functions creates cutpoints in the observations, which are then used as predictors for the linear model to fit a step function.

```{r}
table(cut(age, 4))
fit <- lm(wage ~ cut(age, 4), data = Wage)
coef(summary(fit))
```

### 7.2 Splines

We use the `splines` package to run regression splines. 

```{r}
library(splines)
```

We first use [`bs()`](http://bit.ly/R_bs) to generate a basis matrix for a polynomial spline and fit a model with knots at age 25, 40 and 60.

```{r}
fit <- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage)
pred <- predict(fit, newdata = list(age = age.grid), se = TRUE)
plot(age, wage, col = "gray")
lines(age.grid, pred$fit, lwd = 2)
lines(age.grid, pred$fit + 2 * pred$se, lty = "dashed")
lines(age.grid, pred$fit - 2 * pred$se, lty = "dashed")
```

Alternatively, the [`df()`](http://bit.ly/R_dist) function can be used to produce a spline fit with knots at uniform intervals.

```{r}
dim(bs(age, knots = c(25, 40, 60)))
dim(bs(age, df = 6))
attr(bs(age, df = 6), "knots")
```

```{r}
plot(age, wage, col = "gray")
fit2 <- lm(wage ~ ns(age, df = 4), data = Wage)
pred2 <- predict(fit2, newdata = list(age = age.grid), se = TRUE)
lines(age.grid, pred2$fit, col = "red", lwd = 2)
```

We can fit a smoothing spline using [`smooth.spline()`](http://bit.ly/R_smooth_spline)

```{r}
plot(age, wage, xlim = agelims, cex = 0.5, col = "darkgrey")
title(" Smoothing Spline ")
fit <- smooth.spline(age, wage, df = 16)
fit2 <- smooth.spline(age, wage, cv = TRUE)
fit2$df
lines(fit, col = "red", lwd = 2)
lines(fit2, col = "blue", lwd = 2)
legend("topright", legend = c("16 DF", "6.8 DF"), col = c("red", "blue"), lty = 1, lwd = 2, cex = 0.8)
```

We can use [`loess()`](http://bit.ly/R_loess) function for a local polynomial regression.

```{r}
plot(age, wage, xlim = agelims, cex = 0.5, col = "darkgrey")
title(" Local Regression ")
fit <- loess(wage ~ age, span = 0.2, data = Wage)
fit2 <- loess(wage ~ age, span = 0.5, data = Wage)
lines(age.grid, predict(fit, data.frame(age = age.grid)), col = "red", lwd = 2)
lines(age.grid, predict(fit2, data.frame(age = age.grid)), col = "blue", lwd = 2)
legend("topright", legend = c("Span=0.2", "Span=0.5"), col = c("red", "blue"), lty = 1, lwd = 2, cex = 0.8)
```

### 7.3 GAMs

We can fit a GAM with [`lm()`](http://bit.ly/R_lm) when an appropriate basis function can used.

```{r}
gam1 <- lm(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage)
```

However, the `gam` package offers a general solution to fitting GAMs and is especially useful when splines cannot be easily expressed in terms of basis functions.

```{r}
library(gam)
gam.m3 <- gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage)
```

The [`plot()`](http://bit.ly/R_plot) function behaves the same way as it does with [`lm()`](http://bit.ly/R_lm) and [`glm()`](http://bit.ly/R_glm).

```{r}
par(mfrow = c(1, 3))
plot(gam.m3, se = TRUE, col = "blue")
```

```{r}
plot.gam(gam1, se = TRUE, col = "red")
```

We can use [`anova()`](http://bit.ly/R_anova) to find the best performing model.

```{r}
gam.m1 <- gam(wage ~ s(age, 5) + education, data = Wage)
gam.m2 <- gam(wage ~ year + s(age, 5) + education, data = Wage)
anova(gam.m1, gam.m2, gam.m3, test = "F")
```

And use [`summary()`](http://bit.ly/R_summary) to generate a summary of the fitted model.

```{r}
summary(gam.m3)
```

We can make predictions using [`predict()`](http://bit.ly/R_predict) just as we did with linear and logistic regressions.

```{r}
preds <- predict(gam.m2, newdata = Wage)
```

We can use a loess fit as a smoothing term in a GAM formula using the [`lo()`](http://bit.ly/R_gam_lo) 

```{r}
gam.lo <- gam(wage ~ s(year, df = 4) + lo(age, span = 0.7) + education, data = Wage)
plot.gam(gam.lo, se = TRUE, col = "green")
```

```{r}
gam.lo.i <- gam(wage ~ lo(year, age, span = 0.5) + education, data = Wage)
```

The `akima` package can be used to plot two-dimensional surface plots. Make sure the package is part of your R installation or install it with [`install.packages()`](http://bit.ly/R_install_packages) if necessary.

```{r}
library(akima)
plot(gam.lo.i)
```

The [`gam()`](http://bit.ly/R_gam) function also allows fitting logistic regression GAM with the `family = binomial` argument.

```{r}
gam.lr <- gam(I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, data = Wage)
par(mfrow = c(1, 3))
plot(gam.lr, se = TRUE, col = "green")
```

```{r}
table(education, I(wage > 250))
```

```{r}
gam.lr.s <- gam(I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, data = Wage, subset = (education != "1. < HS Grad"))
plot(gam.lr.s, se = TRUE, col = "green")
```
