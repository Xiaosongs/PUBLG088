---
title: "Assignment 7 - Nonlinear Models"
author: "Slava Mikhaylov"
output: html_document
---

Assignments for the course focus on practical aspects of the concepts covered in the lectures. Assignments are based on the material covered in James et al. You will start working on the assignment in the lab sessions after the lectures, but may need to finish them after class.

You will have two days to work on the assignments, submitting them via Moodle by 7pm on Thursday. We will subsequently open up solutions to the problem sets. 

### Exercise 7.1

In this exercise, you will further analyze the `Wage` dataset coming with the `ISLR` package. 

(a) Perform polynomial regression to predict `wage` using `age`. Use cross-validation to select the optimal degree $d$ for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data.

**Load $Wage$ dataset. Keep an array of all cross-validation errors. We are performing K-fold cross validation with $K=10$.**

```{r}
set.seed(1)
library(ISLR)
library(boot)

all.deltas <-  rep(NA, 10)

for (i in 1:10) {
  glm.fit <-  glm(wage ~ poly(age, i), data = Wage)
  all.deltas[i] <-  cv.glm(Wage, glm.fit, K=10)$delta[2]
}

plot(1:10, all.deltas, xlab = "Degree", ylab = "CV error", type = "l", pch = 20, lwd = 2, ylim = c(1590, 1700))
min.point <-  min(all.deltas)
sd.points <-  sd(all.deltas)
abline(h = min.point + 0.2 * sd.points, col = "red", lty = "dashed")
abline(h = min.point - 0.2 * sd.points, col = "red", lty = "dashed")
legend("topright", "0.2-standard deviation lines", lty = "dashed", col="red")
```

**The cv-plot with standard deviation lines show that $d=3$ is the smallest degree giving reasonably small cross-validation error.**

**We now find best degree using Anova.**

```{r}
fit.1 <-  lm(wage ~ poly(age, 1), data = Wage)
fit.2 <-  lm(wage ~ poly(age, 2), data = Wage)
fit.3 <-  lm(wage ~ poly(age, 3), data = Wage)
fit.4 <-  lm(wage ~ poly(age, 4), data = Wage)
fit.5 <-  lm(wage ~ poly(age, 5), data = Wage)
fit.6 <-  lm(wage ~ poly(age, 6), data = Wage)
fit.7 <-  lm(wage ~ poly(age, 7), data = Wage)
fit.8 <-  lm(wage ~ poly(age, 8), data = Wage)
fit.9 <-  lm(wage ~ poly(age, 9), data = Wage)
fit.10 <- lm(wage ~ poly(age, 10), data = Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5, fit.6, fit.7, fit.8, fit.9, fit.10)
```

**Anova shows that all polynomials above degree $3$ are insignificant at $1%$ significance level.**

**We now plot the polynomial prediction on the data**

```{r}
plot(wage ~ age, data = Wage, col = "darkgrey")
agelims <-  range(Wage$age)
age.grid <-  seq(from = agelims[1], to = agelims[2])
lm.fit <-  lm(wage ~ poly(age, 3), data = Wage)
lm.pred <-  predict(lm.fit, data.frame(age = age.grid))
lines(age.grid, lm.pred, col = "blue", lwd = 2)
```


(b) Fit a step function to predict `wage` using `age`, and perform cross-validation to choose the optimal number of cuts. Make a plot of the fit obtained.

**We use cut points of up to 10.**

```{r}
all.cvs <-  rep(NA, 10)

for (i in 2:10) {
  Wage$age.cut <-  cut(Wage$age, i)
  lm.fit <-  glm(wage ~ age.cut, data = Wage)
  all.cvs[i] <-  cv.glm(Wage, lm.fit, K = 10)$delta[2]
}

plot(2:10, all.cvs[-1], xlab = "Number of cuts", ylab = "CV error", 
     type = "l", pch = 20, lwd = 2)
```

**The cross validation shows that test error is minimum for $k=8$ cuts.**

**We now train the entire data with step function using $8$ cuts and plot it.**

```{r}
lm.fit <-  glm(wage ~ cut(age, 8), data = Wage)
agelims <-  range(Wage$age)
age.grid <-  seq(from = agelims[1], to = agelims[2])
lm.pred <-  predict(lm.fit, data.frame(age = age.grid))
plot(wage ~ age, data = Wage, col = "darkgrey")
lines(age.grid, lm.pred, col = "red", lwd = 2)
```


### Exercise 7.2

The `Wage` data set contains a number of other features that we haven't yet covered, such as marital status (`maritl`), job class (`jobclass`), and others. Explore the relationships between some of these other predictors and `wage`, and use non-linear fitting techniques in order to fit flexible models to the data. Create plots of the results obtained, and write a summary of your findings.


```{r}
library(ISLR)
set.seed(1)
```

```{r,fig.width=16}
summary(Wage$maritl)
summary(Wage$jobclass)
par(mfrow=c(1,2))
plot(Wage$maritl, Wage$wage)
plot(Wage$jobclass, Wage$wage)
```

**It appears a married couple makes more money on average than other groups. It also appears that Informational jobs are higher-wage than Industrial jobs on average.**

#### Polynomial and Step functions

```{r}
fit <-  lm(wage ~ maritl, data = Wage)
deviance(fit)
fit <-  lm(wage ~ jobclass, data = Wage)
deviance(fit)
fit <-  lm(wage ~ maritl + jobclass, data = Wage)
deviance(fit)
```

#### Splines

**Unable to fit splines on categorical variables.**

#### GAMs
```{r}
library(gam)
fit <-  gam(wage ~ maritl + jobclass + s(age,4), data = Wage)
deviance(fit)
```

**Without more advanced techniques, we cannot fit splines to categorical variables (factors). `maritl` and `jobclass` do add statistically significant improvements to the previously discussed models.**


### Exercise 7.3

This question uses the variables `dis` (the weighted mean of distances to five Boston employment centers) and `nox` (nitrogen oxides concentration in parts per 10 million) from the `Boston` data available as part of the `MASS` package. We will treat `dis` as the predictor and `nox` as the response.

**Load the Boston dataset**

```{r}
set.seed(1)
library(MASS)
attach(Boston)
```


(a) Use the `poly()` function to fit a cubic polynomial regression to predict `nox` using `dis`. Report the regression output, and plot the resulting data and polynomial fits.

```{r}
lm.fit <-  lm(nox ~ poly(dis, 3), data = Boston)
summary(lm.fit)
dislim <-  range(dis)
dis.grid <-  seq(from = dislim[1], to = dislim[2], by = 0.1)
lm.pred <-  predict(lm.fit, list(dis = dis.grid))
plot(nox ~ dis, data = Boston, col = "darkgrey")
lines(dis.grid, lm.pred, col = "red", lwd = 2)
```

**Summary shows that all polynomial terms are significant while predicting nox using dis. Plot shows a smooth curve fitting the data fairly well.**

(b) Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.

**We plot polynomials of degrees 1 to 10 and save train RSS.**

```{r}
all.rss <-  rep(NA, 10)

for (i in 1:10) {
  lm.fit <-  lm(nox ~ poly(dis, i), data = Boston)
  all.rss[i] <-  sum(lm.fit$residuals^2)
}

all.rss
```

**As expected, train RSS monotonically decreases with degree of polynomial.** 

(c) Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results.

**We use a 10-fold cross validation to pick the best polynomial degree.**

```{r}
library(boot)
all.deltas <-  rep(NA, 10)

for (i in 1:10) {
  glm.fit <-  glm(nox ~ poly(dis, i), data = Boston)
  all.deltas[i] <-  cv.glm(Boston, glm.fit, K = 10)$delta[2]
}

plot(1:10, all.deltas, xlab = "Degree", ylab = "CV error", 
     type = "l", pch = 20, lwd = 2)
```

**A 10-fold CV shows that the CV error reduces as we increase degree from 1 to 3, stay almost constant till degree 5, and the starts increasing for higher degrees. We pick 4 as the best polynomial degree.**

(d) Use the `bs()` function to fit a regression spline to predict `nox` using `dis`. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.

**We see that `dis` has limits of about 1 and 13 respectively. We split this range in roughly equal 4 intervals and establish knots at $[4, 7, 11]$. Note: `bs()` function in R expects either `df` or `knots` argument. If both are specified, `knots` are ignored.**

```{r}
library(splines)
sp.fit <-  lm(nox ~ bs(dis, df = 4, knots = c(4, 7, 11)), data = Boston)
summary(sp.fit)
sp.pred <-  predict(sp.fit, list(dis = dis.grid))
plot(nox ~ dis, data = Boston, col = "darkgrey")
lines(dis.grid, sp.pred, col = "red", lwd = 2)
```

**The summary shows that all terms in spline fit are significant. Plot shows that the spline fits data well except at the extreme values of $dis$, (especially $dis > 10$).** 

(e) Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.

**We fit regression splines with dfs between 3 and 16.** 

```{r}
all.cv <-  rep(NA, 16)

for (i in 3:16) {
  lm.fit <-  lm(nox ~ bs(dis, df = i), data = Boston)
  all.cv[i] <-  sum(lm.fit$residuals^2)
}

all.cv[-c(1, 2)]
```

**Train RSS monotonically decreases till df=14 and then slightly increases for df=15 and df=16.**


### Exercise 7.4

This question relates to the `College` dataset from the `ISLR` package.

(a) Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors.

```{r}
set.seed(1)
library(ISLR)
library(leaps)
attach(College)
train <-  sample(length(Outstate), length(Outstate)/2)
test <-  -train
College.train <-  College[train, ]
College.test <-  College[test, ]
reg.fit <-  regsubsets(Outstate~., data=College.train, nvmax=17,
                       method="forward")
reg.summary <-  summary(reg.fit)
par(mfrow=c(1, 3))
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
min.cp <-  min(reg.summary$cp)
std.cp <-  sd(reg.summary$cp)
abline(h=min.cp+0.2*std.cp, col="red", lty=2)
abline(h=min.cp-0.2*std.cp, col="red", lty=2)
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
min.bic <-  min(reg.summary$bic)
std.bic <-  sd(reg.summary$bic)
abline(h=min.bic+0.2*std.bic, col="red", lty=2)
abline(h=min.bic-0.2*std.bic, col="red", lty=2)
plot(reg.summary$adjr2,xlab="Number of Variables",
     ylab="Adjusted R2",type='l', ylim=c(0.4, 0.84))
max.adjr2 <-  max(reg.summary$adjr2)
std.adjr2 <-  sd(reg.summary$adjr2)
abline(h=max.adjr2+0.2*std.adjr2, col="red", lty=2)
abline(h=max.adjr2-0.2*std.adjr2, col="red", lty=2)
```

**All cp, BIC and adjr2 scores show that size 6 is the minimum size for the subset for which the scores are withing 0.2 standard deviations of optimum. We pick 6 as the best subset size and find best 6 variables using entire data.**

```{r}
reg.fit <-  regsubsets(Outstate ~ . , data=College, method="forward")
coefi <-  coef(reg.fit, id=6)
names(coefi)
```


(b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings.

```{r}
library(gam)
gam.fit <-  gam(Outstate ~ Private + s(Room.Board, df=2) + 
                  s(PhD, df=2) + s(perc.alumni, df=2) + 
                  s(Expend, df=5) + s(Grad.Rate, df=2),
                data=College.train)
par(mfrow=c(2, 3))
plot(gam.fit, se=TRUE, col="blue")
```

(c) Evaluate the model obtained on the test set, and explain the results obtained.

```{r}
gam.pred <-  predict(gam.fit, College.test)
gam.err <-  mean((College.test$Outstate - gam.pred)^2)
gam.err
gam.tss <-  mean((College.test$Outstate - mean(College.test$Outstate))^2)
test.rss <-  1 - gam.err / gam.tss
test.rss
```

**We obtain a test R-squared of 0.77 using GAM with 6 predictors. This is a slight improvement over a test RSS of 0.74 obtained using OLS.** 

(d) For which variables, if any, is there evidence of a non-linear relationship with the response?

```{r}
summary(gam.fit)
```

**Non-parametric Anova test shows a strong evidence of non-linear relationship between response and Expend, and a moderately strong non-linear relationship (using p value of 0.05) between response and Grad.Rate or PhD.** 
